{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings from pandas\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Memory management\n",
    "import gc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Aggregate Numeric Data\n",
    "This groups data by the group_var and calculates mean, max, min, and sum. It will only be applied to numeric data by default in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_numeric(df, parent_var, df_name):\n",
    "    \"\"\"\n",
    "    Groups and aggregates the numeric values in a child dataframe\n",
    "    by the parent variable.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        df (dataframe): \n",
    "            the child dataframe to calculate the statistics on\n",
    "        parent_var (string): \n",
    "            the parent variable used for grouping and aggregating\n",
    "        df_name (string): \n",
    "            the variable used to rename the columns\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        agg (dataframe): \n",
    "            a dataframe with the statistics aggregated by the `parent_var` for \n",
    "            all numeric columns. Each observation of the parent variable will have \n",
    "            one row in the dataframe with the parent variable as the index. \n",
    "            The columns are also renamed using the `df_name`. Columns with all duplicate\n",
    "            values are removed. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove id variables other than grouping variable\n",
    "    for col in df:\n",
    "        if col != parent_var and 'SK_ID' in col:\n",
    "            df = df.drop(columns = col)\n",
    "            \n",
    "    # Only want the numeric variables\n",
    "    parent_ids = df[parent_var].copy()\n",
    "    numeric_df = df.select_dtypes('number').copy()\n",
    "    numeric_df[parent_var] = parent_ids\n",
    "\n",
    "    # Group by the specified variable and calculate the statistics\n",
    "    agg = numeric_df.groupby(parent_var).agg(['count', 'mean', 'max', 'min', 'sum'])\n",
    "\n",
    "    # Need to create new column names\n",
    "    columns = []\n",
    "\n",
    "    # Iterate through the variables names\n",
    "    for var in agg.columns.levels[0]:\n",
    "        if var != parent_var:\n",
    "            # Iterate through the stat names\n",
    "            for stat in agg.columns.levels[1]:\n",
    "                # Make a new column name for the variable and stat\n",
    "                columns.append('%s_%s_%s' % (df_name, var, stat))\n",
    "    \n",
    "    agg.columns = columns\n",
    "    \n",
    "    # Remove the columns with all redundant values\n",
    "    _, idx = np.unique(agg, axis = 1, return_index=True)\n",
    "    agg = agg.iloc[:, idx]\n",
    "    \n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to Calculate Categorical Counts\n",
    "This function calculates the occurrences (counts) of each category in a categorical variable for each client. It also calculates the normed count, which is the count for a category divided by the total counts for all categories in a categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_categorical(df, parent_var, df_name):\n",
    "    \"\"\"\n",
    "    Aggregates the categorical features in a child dataframe\n",
    "    for each observation of the parent variable.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    df : dataframe \n",
    "        The dataframe to calculate the value counts for.\n",
    "        \n",
    "    parent_var : string\n",
    "        The variable by which to group and aggregate the dataframe. For each unique\n",
    "        value of this variable, the final dataframe will have one row\n",
    "        \n",
    "    df_name : string\n",
    "        Variable added to the front of column names to keep track of columns\n",
    "\n",
    "    \n",
    "    Return\n",
    "    --------\n",
    "    categorical : dataframe\n",
    "        A dataframe with aggregated statistics for each observation of the parent_var\n",
    "        The columns are also renamed and columns with duplicate values are removed.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Select the categorical columns\n",
    "    categorical = pd.get_dummies(df.select_dtypes('category'))\n",
    "\n",
    "    # Make sure to put the identifying id on the column\n",
    "    categorical[parent_var] = df[parent_var]\n",
    "\n",
    "    # Groupby the group var and calculate the sum and mean\n",
    "    categorical = categorical.groupby(parent_var).agg(['sum', 'count', 'mean'])\n",
    "    \n",
    "    column_names = []\n",
    "    \n",
    "    # Iterate through the columns in level 0\n",
    "    for var in categorical.columns.levels[0]:\n",
    "        # Iterate through the stats in level 1\n",
    "        for stat in ['sum', 'count', 'mean']:\n",
    "            # Make a new column name\n",
    "            column_names.append('%s_%s_%s' % (df_name, var, stat))\n",
    "    \n",
    "    categorical.columns = column_names\n",
    "    \n",
    "    # Remove duplicate columns by values\n",
    "    _, idx = np.unique(categorical, axis = 1, return_index = True)\n",
    "    categorical = categorical.iloc[:, idx]\n",
    "    \n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the disribution of a variable colored by value of the target\n",
    "def kde_target(var_name, df):\n",
    "    \n",
    "    # Calculate the correlation coefficient between the new variable and the target\n",
    "    corr = df['TARGET'].corr(df[var_name])\n",
    "    \n",
    "    # Calculate medians for repaid vs not repaid\n",
    "    avg_repaid = df.ix[df['TARGET'] == 0, var_name].median()\n",
    "    avg_not_repaid = df.ix[df['TARGET'] == 1, var_name].median()\n",
    "    \n",
    "    plt.figure(figsize = (12, 6))\n",
    "    \n",
    "    # Plot the distribution for target == 0 and target == 1\n",
    "    sns.kdeplot(df.ix[df['TARGET'] == 0, var_name], label = 'TARGET == 0')\n",
    "    sns.kdeplot(df.ix[df['TARGET'] == 1, var_name], label = 'TARGET == 1')\n",
    "    \n",
    "    # label the plot\n",
    "    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n",
    "    plt.legend();\n",
    "    \n",
    "    # print out the correlation\n",
    "    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr))\n",
    "    # Print out average values\n",
    "    print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid)\n",
    "    print('Median value for loan that was repaid =     %0.4f' % avg_repaid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Convert Data Types\n",
    "This will help reduce memory usage by using more efficient types for the variables. For example category is often a better type than object (unless the number of unique categories is close to the number of rows in the dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def return_size(df):\n",
    "    \"\"\"Return size of dataframe in gigabytes\"\"\"\n",
    "    return round(sys.getsizeof(df) / 1e9, 2)\n",
    "\n",
    "def convert_types(df, print_info = False):\n",
    "    \n",
    "    original_memory = df.memory_usage().sum()\n",
    "    \n",
    "    # Iterate through each column\n",
    "    for c in df:\n",
    "        \n",
    "        # Convert ids and booleans to integers\n",
    "        if ('SK_ID' in c):\n",
    "            df[c] = df[c].fillna(0).astype(np.int32)\n",
    "            \n",
    "        # Convert objects to category\n",
    "        elif (df[c].dtype == 'object') and (df[c].nunique() < df.shape[0]):\n",
    "            df[c] = df[c].astype('category')\n",
    "        \n",
    "        # Booleans mapped to integers\n",
    "        elif list(df[c].unique()) == [1, 0]:\n",
    "            df[c] = df[c].astype(bool)\n",
    "        \n",
    "        # Float64 to float32\n",
    "        elif df[c].dtype == float:\n",
    "            df[c] = df[c].astype(np.float32)\n",
    "            \n",
    "        # Int64 to int32\n",
    "        elif df[c].dtype == int:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "        \n",
    "    new_memory = df.memory_usage().sum()\n",
    "    \n",
    "    if print_info:\n",
    "        print(f'Original Memory Usage: {round(original_memory / 1e9, 2)} gb.')\n",
    "        print(f'New Memory Usage: {round(new_memory / 1e9, 2)} gb.')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### previous_application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Memory Usage: 0.49 gb.\n",
      "New Memory Usage: 0.18 gb.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_PREV</th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_APPLICATION</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_DOWN_PAYMENT</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>WEEKDAY_APPR_PROCESS_START</th>\n",
       "      <th>HOUR_APPR_PROCESS_START</th>\n",
       "      <th>...</th>\n",
       "      <th>NAME_SELLER_INDUSTRY</th>\n",
       "      <th>CNT_PAYMENT</th>\n",
       "      <th>NAME_YIELD_GROUP</th>\n",
       "      <th>PRODUCT_COMBINATION</th>\n",
       "      <th>DAYS_FIRST_DRAWING</th>\n",
       "      <th>DAYS_FIRST_DUE</th>\n",
       "      <th>DAYS_LAST_DUE_1ST_VERSION</th>\n",
       "      <th>DAYS_LAST_DUE</th>\n",
       "      <th>DAYS_TERMINATION</th>\n",
       "      <th>NFLAG_INSURED_ON_APPROVAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2030495</td>\n",
       "      <td>271877</td>\n",
       "      <td>Consumer loans</td>\n",
       "      <td>1730.430054</td>\n",
       "      <td>17145.0</td>\n",
       "      <td>17145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17145.0</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>Connectivity</td>\n",
       "      <td>12.0</td>\n",
       "      <td>middle</td>\n",
       "      <td>POS mobile with interest</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>-37.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2802425</td>\n",
       "      <td>108129</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>25188.615234</td>\n",
       "      <td>607500.0</td>\n",
       "      <td>679671.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>607500.0</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>XNA</td>\n",
       "      <td>36.0</td>\n",
       "      <td>low_action</td>\n",
       "      <td>Cash X-Sell: low</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>-134.0</td>\n",
       "      <td>916.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2523466</td>\n",
       "      <td>122040</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>15060.735352</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>136444.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>TUESDAY</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>XNA</td>\n",
       "      <td>12.0</td>\n",
       "      <td>high</td>\n",
       "      <td>Cash X-Sell: high</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>-271.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2819243</td>\n",
       "      <td>176158</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>47041.335938</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>470790.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>XNA</td>\n",
       "      <td>12.0</td>\n",
       "      <td>middle</td>\n",
       "      <td>Cash X-Sell: middle</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>-482.0</td>\n",
       "      <td>-152.0</td>\n",
       "      <td>-182.0</td>\n",
       "      <td>-177.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1784265</td>\n",
       "      <td>202054</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>31924.394531</td>\n",
       "      <td>337500.0</td>\n",
       "      <td>404055.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>337500.0</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>XNA</td>\n",
       "      <td>24.0</td>\n",
       "      <td>high</td>\n",
       "      <td>Cash Street: high</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_PREV  SK_ID_CURR NAME_CONTRACT_TYPE   AMT_ANNUITY  AMT_APPLICATION  \\\n",
       "0     2030495      271877     Consumer loans   1730.430054          17145.0   \n",
       "1     2802425      108129         Cash loans  25188.615234         607500.0   \n",
       "2     2523466      122040         Cash loans  15060.735352         112500.0   \n",
       "3     2819243      176158         Cash loans  47041.335938         450000.0   \n",
       "4     1784265      202054         Cash loans  31924.394531         337500.0   \n",
       "\n",
       "   AMT_CREDIT  AMT_DOWN_PAYMENT  AMT_GOODS_PRICE WEEKDAY_APPR_PROCESS_START  \\\n",
       "0     17145.0               0.0          17145.0                   SATURDAY   \n",
       "1    679671.0               NaN         607500.0                   THURSDAY   \n",
       "2    136444.5               NaN         112500.0                    TUESDAY   \n",
       "3    470790.0               NaN         450000.0                     MONDAY   \n",
       "4    404055.0               NaN         337500.0                   THURSDAY   \n",
       "\n",
       "   HOUR_APPR_PROCESS_START            ...            NAME_SELLER_INDUSTRY  \\\n",
       "0                       15            ...                    Connectivity   \n",
       "1                       11            ...                             XNA   \n",
       "2                       11            ...                             XNA   \n",
       "3                        7            ...                             XNA   \n",
       "4                        9            ...                             XNA   \n",
       "\n",
       "   CNT_PAYMENT  NAME_YIELD_GROUP       PRODUCT_COMBINATION  \\\n",
       "0         12.0            middle  POS mobile with interest   \n",
       "1         36.0        low_action          Cash X-Sell: low   \n",
       "2         12.0              high         Cash X-Sell: high   \n",
       "3         12.0            middle       Cash X-Sell: middle   \n",
       "4         24.0              high         Cash Street: high   \n",
       "\n",
       "   DAYS_FIRST_DRAWING DAYS_FIRST_DUE DAYS_LAST_DUE_1ST_VERSION  DAYS_LAST_DUE  \\\n",
       "0            365243.0          -42.0                     300.0          -42.0   \n",
       "1            365243.0         -134.0                     916.0       365243.0   \n",
       "2            365243.0         -271.0                      59.0       365243.0   \n",
       "3            365243.0         -482.0                    -152.0         -182.0   \n",
       "4                 NaN            NaN                       NaN            NaN   \n",
       "\n",
       "  DAYS_TERMINATION NFLAG_INSURED_ON_APPROVAL  \n",
       "0            -37.0                       0.0  \n",
       "1         365243.0                       1.0  \n",
       "2         365243.0                       1.0  \n",
       "3           -177.0                       1.0  \n",
       "4              NaN                       NaN  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous = pd.read_csv('../dataset/previous_application.csv')\n",
    "previous = convert_types(previous, print_info=True)\n",
    "previous.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SK_ID_PREV', 'SK_ID_CURR', 'NAME_CONTRACT_TYPE', 'AMT_ANNUITY',\n",
       "       'AMT_APPLICATION', 'AMT_CREDIT', 'AMT_DOWN_PAYMENT', 'AMT_GOODS_PRICE',\n",
       "       'WEEKDAY_APPR_PROCESS_START', 'HOUR_APPR_PROCESS_START',\n",
       "       'FLAG_LAST_APPL_PER_CONTRACT', 'NFLAG_LAST_APPL_IN_DAY',\n",
       "       'RATE_DOWN_PAYMENT', 'RATE_INTEREST_PRIMARY',\n",
       "       'RATE_INTEREST_PRIVILEGED', 'NAME_CASH_LOAN_PURPOSE',\n",
       "       'NAME_CONTRACT_STATUS', 'DAYS_DECISION', 'NAME_PAYMENT_TYPE',\n",
       "       'CODE_REJECT_REASON', 'NAME_TYPE_SUITE', 'NAME_CLIENT_TYPE',\n",
       "       'NAME_GOODS_CATEGORY', 'NAME_PORTFOLIO', 'NAME_PRODUCT_TYPE',\n",
       "       'CHANNEL_TYPE', 'SELLERPLACE_AREA', 'NAME_SELLER_INDUSTRY',\n",
       "       'CNT_PAYMENT', 'NAME_YIELD_GROUP', 'PRODUCT_COMBINATION',\n",
       "       'DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION',\n",
       "       'DAYS_LAST_DUE', 'DAYS_TERMINATION', 'NFLAG_INSURED_ON_APPROVAL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous aggregation shape:  (338857, 80)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>previous_DAYS_DECISION_sum</th>\n",
       "      <th>previous_DAYS_DECISION_min</th>\n",
       "      <th>previous_DAYS_DECISION_mean</th>\n",
       "      <th>previous_DAYS_DECISION_max</th>\n",
       "      <th>previous_DAYS_FIRST_DUE_sum</th>\n",
       "      <th>previous_DAYS_FIRST_DUE_min</th>\n",
       "      <th>previous_DAYS_FIRST_DUE_mean</th>\n",
       "      <th>previous_DAYS_FIRST_DUE_max</th>\n",
       "      <th>previous_DAYS_LAST_DUE_sum</th>\n",
       "      <th>previous_DAYS_LAST_DUE_min</th>\n",
       "      <th>...</th>\n",
       "      <th>previous_DAYS_FIRST_DRAWING_min</th>\n",
       "      <th>previous_DAYS_FIRST_DRAWING_mean</th>\n",
       "      <th>previous_DAYS_FIRST_DRAWING_max</th>\n",
       "      <th>previous_DAYS_FIRST_DRAWING_sum</th>\n",
       "      <th>previous_RATE_INTEREST_PRIMARY_min</th>\n",
       "      <th>previous_RATE_INTEREST_PRIMARY_mean</th>\n",
       "      <th>previous_RATE_INTEREST_PRIMARY_max</th>\n",
       "      <th>previous_RATE_INTEREST_PRIVILEGED_min</th>\n",
       "      <th>previous_RATE_INTEREST_PRIVILEGED_mean</th>\n",
       "      <th>previous_RATE_INTEREST_PRIVILEGED_max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>-1740</td>\n",
       "      <td>-1740</td>\n",
       "      <td>-1740.0</td>\n",
       "      <td>-1740</td>\n",
       "      <td>-1709.0</td>\n",
       "      <td>-1709.0</td>\n",
       "      <td>-1709.000000</td>\n",
       "      <td>-1709.0</td>\n",
       "      <td>-1619.0</td>\n",
       "      <td>-1619.0</td>\n",
       "      <td>...</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>-606</td>\n",
       "      <td>-606</td>\n",
       "      <td>-606.0</td>\n",
       "      <td>-606</td>\n",
       "      <td>-565.0</td>\n",
       "      <td>-565.0</td>\n",
       "      <td>-565.000000</td>\n",
       "      <td>-565.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>-3915</td>\n",
       "      <td>-2341</td>\n",
       "      <td>-1305.0</td>\n",
       "      <td>-746</td>\n",
       "      <td>-3823.0</td>\n",
       "      <td>-2310.0</td>\n",
       "      <td>-1274.333374</td>\n",
       "      <td>-716.0</td>\n",
       "      <td>-3163.0</td>\n",
       "      <td>-1980.0</td>\n",
       "      <td>...</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>1095729.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>-815</td>\n",
       "      <td>-815</td>\n",
       "      <td>-815.0</td>\n",
       "      <td>-815</td>\n",
       "      <td>-784.0</td>\n",
       "      <td>-784.0</td>\n",
       "      <td>-784.000000</td>\n",
       "      <td>-784.0</td>\n",
       "      <td>-724.0</td>\n",
       "      <td>-724.0</td>\n",
       "      <td>...</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100005</th>\n",
       "      <td>-1072</td>\n",
       "      <td>-757</td>\n",
       "      <td>-536.0</td>\n",
       "      <td>-315</td>\n",
       "      <td>-706.0</td>\n",
       "      <td>-706.0</td>\n",
       "      <td>-706.000000</td>\n",
       "      <td>-706.0</td>\n",
       "      <td>-466.0</td>\n",
       "      <td>-466.0</td>\n",
       "      <td>...</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            previous_DAYS_DECISION_sum  previous_DAYS_DECISION_min  \\\n",
       "SK_ID_CURR                                                           \n",
       "100001                           -1740                       -1740   \n",
       "100002                            -606                        -606   \n",
       "100003                           -3915                       -2341   \n",
       "100004                            -815                        -815   \n",
       "100005                           -1072                        -757   \n",
       "\n",
       "            previous_DAYS_DECISION_mean  previous_DAYS_DECISION_max  \\\n",
       "SK_ID_CURR                                                            \n",
       "100001                          -1740.0                       -1740   \n",
       "100002                           -606.0                        -606   \n",
       "100003                          -1305.0                        -746   \n",
       "100004                           -815.0                        -815   \n",
       "100005                           -536.0                        -315   \n",
       "\n",
       "            previous_DAYS_FIRST_DUE_sum  previous_DAYS_FIRST_DUE_min  \\\n",
       "SK_ID_CURR                                                             \n",
       "100001                          -1709.0                      -1709.0   \n",
       "100002                           -565.0                       -565.0   \n",
       "100003                          -3823.0                      -2310.0   \n",
       "100004                           -784.0                       -784.0   \n",
       "100005                           -706.0                       -706.0   \n",
       "\n",
       "            previous_DAYS_FIRST_DUE_mean  previous_DAYS_FIRST_DUE_max  \\\n",
       "SK_ID_CURR                                                              \n",
       "100001                      -1709.000000                      -1709.0   \n",
       "100002                       -565.000000                       -565.0   \n",
       "100003                      -1274.333374                       -716.0   \n",
       "100004                       -784.000000                       -784.0   \n",
       "100005                       -706.000000                       -706.0   \n",
       "\n",
       "            previous_DAYS_LAST_DUE_sum  previous_DAYS_LAST_DUE_min  \\\n",
       "SK_ID_CURR                                                           \n",
       "100001                         -1619.0                     -1619.0   \n",
       "100002                           -25.0                       -25.0   \n",
       "100003                         -3163.0                     -1980.0   \n",
       "100004                          -724.0                      -724.0   \n",
       "100005                          -466.0                      -466.0   \n",
       "\n",
       "                            ...                    \\\n",
       "SK_ID_CURR                  ...                     \n",
       "100001                      ...                     \n",
       "100002                      ...                     \n",
       "100003                      ...                     \n",
       "100004                      ...                     \n",
       "100005                      ...                     \n",
       "\n",
       "            previous_DAYS_FIRST_DRAWING_min  previous_DAYS_FIRST_DRAWING_mean  \\\n",
       "SK_ID_CURR                                                                      \n",
       "100001                             365243.0                          365243.0   \n",
       "100002                             365243.0                          365243.0   \n",
       "100003                             365243.0                          365243.0   \n",
       "100004                             365243.0                          365243.0   \n",
       "100005                             365243.0                          365243.0   \n",
       "\n",
       "            previous_DAYS_FIRST_DRAWING_max  previous_DAYS_FIRST_DRAWING_sum  \\\n",
       "SK_ID_CURR                                                                     \n",
       "100001                             365243.0                         365243.0   \n",
       "100002                             365243.0                         365243.0   \n",
       "100003                             365243.0                        1095729.0   \n",
       "100004                             365243.0                         365243.0   \n",
       "100005                             365243.0                         365243.0   \n",
       "\n",
       "            previous_RATE_INTEREST_PRIMARY_min  \\\n",
       "SK_ID_CURR                                       \n",
       "100001                                     NaN   \n",
       "100002                                     NaN   \n",
       "100003                                     NaN   \n",
       "100004                                     NaN   \n",
       "100005                                     NaN   \n",
       "\n",
       "            previous_RATE_INTEREST_PRIMARY_mean  \\\n",
       "SK_ID_CURR                                        \n",
       "100001                                      NaN   \n",
       "100002                                      NaN   \n",
       "100003                                      NaN   \n",
       "100004                                      NaN   \n",
       "100005                                      NaN   \n",
       "\n",
       "            previous_RATE_INTEREST_PRIMARY_max  \\\n",
       "SK_ID_CURR                                       \n",
       "100001                                     NaN   \n",
       "100002                                     NaN   \n",
       "100003                                     NaN   \n",
       "100004                                     NaN   \n",
       "100005                                     NaN   \n",
       "\n",
       "            previous_RATE_INTEREST_PRIVILEGED_min  \\\n",
       "SK_ID_CURR                                          \n",
       "100001                                        NaN   \n",
       "100002                                        NaN   \n",
       "100003                                        NaN   \n",
       "100004                                        NaN   \n",
       "100005                                        NaN   \n",
       "\n",
       "            previous_RATE_INTEREST_PRIVILEGED_mean  \\\n",
       "SK_ID_CURR                                           \n",
       "100001                                         NaN   \n",
       "100002                                         NaN   \n",
       "100003                                         NaN   \n",
       "100004                                         NaN   \n",
       "100005                                         NaN   \n",
       "\n",
       "            previous_RATE_INTEREST_PRIVILEGED_max  \n",
       "SK_ID_CURR                                         \n",
       "100001                                        NaN  \n",
       "100002                                        NaN  \n",
       "100003                                        NaN  \n",
       "100004                                        NaN  \n",
       "100005                                        NaN  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate aggregate statistics for each numeric column\n",
    "previous_agg = agg_numeric(previous, 'SK_ID_CURR', 'previous')\n",
    "print('Previous aggregation shape: ', previous_agg.shape)\n",
    "previous_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate value counts for each categorical column\n",
    "previous_counts = agg_categorical(previous, 'SK_ID_CURR', 'previous')\n",
    "print('Previous counts shape: ', previous_counts.shape)\n",
    "previous_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../dataset/application_train.csv')\n",
    "train = convert_types(train)\n",
    "test = pd.read_csv('../dataset/application_test.csv')\n",
    "test = convert_types(test)\n",
    "\n",
    "# Merge in the previous information\n",
    "train = train.merge(previous_counts, on ='SK_ID_CURR', how = 'left')\n",
    "train = train.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "test = test.merge(previous_counts, on ='SK_ID_CURR', how = 'left')\n",
    "test = test.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "# Remove variables to free memory\n",
    "gc.enable()\n",
    "del previous, previous_agg, previous_counts\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Calculate Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column# Funct \n",
    "def missing_values_table(df, print_info = False):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        if print_info:\n",
    "            # Print some summary information\n",
    "            print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "                \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "                  \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_columns(train, test, threshold = 90):\n",
    "    # Calculate missing stats for train and test (remember to calculate a percent!)\n",
    "    train_miss = pd.DataFrame(train.isnull().sum())\n",
    "    train_miss['percent'] = 100 * train_miss[0] / len(train)\n",
    "    \n",
    "    test_miss = pd.DataFrame(test.isnull().sum())\n",
    "    test_miss['percent'] = 100 * test_miss[0] / len(test)\n",
    "    \n",
    "    # list of missing columns for train and test\n",
    "    missing_train_columns = list(train_miss.index[train_miss['percent'] > threshold])\n",
    "    missing_test_columns = list(test_miss.index[test_miss['percent'] > threshold])\n",
    "    \n",
    "    # Combine the two lists together\n",
    "    missing_columns = list(set(missing_train_columns + missing_test_columns))\n",
    "    \n",
    "    # Print information\n",
    "    print('There are %d columns with greater than %d%% missing values.' % (len(missing_columns), threshold))\n",
    "    \n",
    "    # Drop the missing columns and return\n",
    "    train = train.drop(columns = missing_columns)\n",
    "    test = test.drop(columns = missing_columns)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = remove_missing_columns(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying to More Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Aggregate Stats at the Client Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_client(df, group_vars, df_names):\n",
    "    \"\"\"Aggregate a dataframe with data at the loan level \n",
    "    at the client level\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): data at the loan level\n",
    "        group_vars (list of two strings): grouping variables for the loan \n",
    "        and then the client (example ['SK_ID_PREV', 'SK_ID_CURR'])\n",
    "        names (list of two strings): names to call the resulting columns\n",
    "        (example ['cash', 'client'])\n",
    "        \n",
    "    Returns:\n",
    "        df_client (dataframe): aggregated numeric stats at the client level. \n",
    "        Each client will have a single row with all the numeric data aggregated\n",
    "    \"\"\"\n",
    "    \n",
    "    # Aggregate the numeric columns\n",
    "    df_agg = agg_numeric(df, parent_var = group_vars[0], df_name = df_names[0])\n",
    "    \n",
    "    # If there are categorical variables\n",
    "    if any(df.dtypes == 'category'):\n",
    "    \n",
    "        # Count the categorical columns\n",
    "        df_counts = agg_categorical(df, parent_var = group_vars[0], df_name = df_names[0])\n",
    "\n",
    "        # Merge the numeric and categorical\n",
    "        df_by_loan = df_counts.merge(df_agg, on = group_vars[0], how = 'outer')\n",
    "        print(df_by_loan)\n",
    "\n",
    "        gc.enable()\n",
    "        del df_agg, df_counts\n",
    "        gc.collect()\n",
    "\n",
    "        # Merge to get the client id in dataframe\n",
    "        df_by_loan = df_by_loan.merge(df[[group_vars[0], group_vars[1]]], on = group_vars[0], how = 'left')\n",
    "\n",
    "        # Remove the loan id\n",
    "        df_by_loan = df_by_loan.drop(columns = [group_vars[0]])\n",
    "\n",
    "        # Aggregate numeric stats by column\n",
    "        df_by_client = agg_numeric(df_by_loan, parent_var = group_vars[1], df_name = df_names[1])\n",
    "        print(df_by_client.head())\n",
    "        \n",
    "    # No categorical variables\n",
    "    else:\n",
    "        # Merge to get the client id in dataframe\n",
    "        df_by_loan = df_agg.merge(df[[group_vars[0], group_vars[1]]], on = group_vars[0], how = 'left')\n",
    "        \n",
    "        gc.enable()\n",
    "        del df_agg\n",
    "        gc.collect()\n",
    "        \n",
    "        # Remove the loan id\n",
    "        df_by_loan = df_by_loan.drop(columns = [group_vars[0]])\n",
    "        \n",
    "        # Aggregate numeric stats by column\n",
    "        df_by_client = agg_numeric(df_by_loan, parent_var = group_vars[1], df_name = df_names[1])\n",
    "        \n",
    "    # Memory management\n",
    "    gc.enable()\n",
    "    del df, df_by_loan\n",
    "    gc.collect()\n",
    "\n",
    "    return df_by_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monthly Cash Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash = pd.read_csv('../dataset/POS_CASH_balance.csv')\n",
    "cash = convert_types(cash, print_info=True)\n",
    "cash.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_by_client = aggregate_client(cash, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['cash', 'client'])\n",
    "cash_by_client.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cash by Client Shape: ', cash_by_client.shape)\n",
    "train = train.merge(cash_by_client, on = 'SK_ID_CURR', how = 'left')\n",
    "test = test.merge(cash_by_client, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "gc.enable()\n",
    "del cash, cash_by_client\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = remove_missing_columns(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit = pd.read_csv('../dataset/credit_card_balance.csv')\n",
    "credit = convert_types(credit, print_info = True)\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monthly Credit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit = pd.read_csv('../dataset/credit_card_balance.csv')\n",
    "credit = convert_types(credit, print_info = True)\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_by_client = aggregate_client(credit, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['credit', 'client'])\n",
    "credit_by_client.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Credit by client shape: ', credit_by_client.shape)\n",
    "\n",
    "train = train.merge(credit_by_client, on = 'SK_ID_CURR', how = 'left')\n",
    "test = test.merge(credit_by_client, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "gc.enable()\n",
    "del credit, credit_by_client\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = remove_missing_columns(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Installment Payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installments = pd.read_csv('../dataset/installments_payments.csv')\n",
    "installments = convert_types(installments, print_info = True)\n",
    "installments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installments_by_client = aggregate_client(installments, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['installments', 'client'])\n",
    "installments_by_client.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Installments by client shape: ', installments_by_client.shape)\n",
    "\n",
    "train = train.merge(installments_by_client, on = 'SK_ID_CURR', how = 'left')\n",
    "test = test.merge(installments_by_client, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "gc.enable()\n",
    "del installments, installments_by_client\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = remove_missing_columns(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final Training Shape: ', train.shape)\n",
    "print('Final Testing Shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Final training size: {return_size(train)}')\n",
    "print(f'Final testing size: {return_size(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_previous_raw.csv', index = False, chunksize = 500)\n",
    "test.to_csv('test_previous_raw.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(features, test_features, encoding = 'ohe', n_folds = 5):\n",
    "    \n",
    "    \"\"\"Train and test a light gradient boosting model using\n",
    "    cross validation. \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        features (pd.DataFrame): \n",
    "            dataframe of training features to use \n",
    "            for training a model. Must include the TARGET column.\n",
    "        test_features (pd.DataFrame): \n",
    "            dataframe of testing features to use\n",
    "            for making predictions with the model. \n",
    "        encoding (str, default = 'ohe'): \n",
    "            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n",
    "            n_folds (int, default = 5): number of folds to use for cross validation\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        submission (pd.DataFrame): \n",
    "            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n",
    "            predicted by the model.\n",
    "        feature_importances (pd.DataFrame): \n",
    "            dataframe with the feature importances from the model.\n",
    "        valid_metrics (pd.DataFrame): \n",
    "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the ids\n",
    "    train_ids = features['SK_ID_CURR']\n",
    "    test_ids = test_features['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = features['TARGET']\n",
    "    \n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n",
    "    \n",
    "    \n",
    "    # One Hot Encoding\n",
    "    if encoding == 'ohe':\n",
    "        features = pd.get_dummies(features)\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "        \n",
    "        # Align the dataframes by the columns\n",
    "        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
    "        \n",
    "        # No categorical indices to record\n",
    "        cat_indices = 'auto'\n",
    "    \n",
    "    # Integer label encoding\n",
    "    elif encoding == 'le':\n",
    "        \n",
    "        # Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "        \n",
    "        # List for storing categorical indices\n",
    "        cat_indices = []\n",
    "        \n",
    "        # Iterate through each column\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == 'object':\n",
    "                # Map the categorical features to integers\n",
    "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
    "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
    "\n",
    "                # Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "    \n",
    "    # Catch error if label encoding scheme is not valid\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
    "        \n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "    \n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "    \n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    \n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = False, random_state = 50)\n",
    "    \n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    \n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "        \n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "        \n",
    "        # Create the model\n",
    "        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n",
    "                                   class_weight = 'balanced', learning_rate = 0.05, \n",
    "                                   reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n",
    "                  early_stopping_rounds = 100, verbose = 200)\n",
    "        \n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "        \n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "        \n",
    "        # Make predictions\n",
    "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
    "        \n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "        \n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "        \n",
    "    # Make the submission dataframe\n",
    "    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
    "    \n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "    \n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "    \n",
    "    # Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "    \n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "    \n",
    "    # Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores}) \n",
    "    \n",
    "    return submission, feature_importances, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission, fi, metrics = model(train, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
